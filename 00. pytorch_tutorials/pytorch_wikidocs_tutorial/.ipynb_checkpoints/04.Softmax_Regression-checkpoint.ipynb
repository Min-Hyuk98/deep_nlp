{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀(Softmax Regression)\n",
    "- 소프트맥스 회귀는 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야 합니다\n",
    "- 소프트맥스의 결과의 각 차원은, 각 클래스가 정답일 확률\n",
    "- H(X)=softmax(WX+B)\n",
    "- LogSoftmax + NLLLoss 또는\n",
    "- Softmax + CrossEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn과 torch.nn.functional의 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True) \n",
    "target = torch.empty(3, dtype=torch.long).random_(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss = torch.nn.CrossEntropyLoss()(output, target) --> class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9639, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_class = nn.CrossEntropyLoss() \n",
    "loss = loss_class(input, target) \n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss = torch.nn.functional.nll_loss(output, target) --> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7231, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(input, target) \n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀의 loss function 구현하기 (low level)\n",
    "- cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([1, 2, 3]) # (1,3)\n",
    "y_hat = F.softmax(z, dim=0) # 행방향에 대해서\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900],\n",
       "        [0.2447],\n",
       "        [0.6652]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([[1], [2], [3]]) # (3,1)\n",
    "y_hat = F.softmax(z, dim=0) # 행방향에 대해서\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1888, 0.1792, 0.2882, 0.1664, 0.1774],\n",
       "        [0.1831, 0.2625, 0.1892, 0.2301, 0.1351],\n",
       "        [0.1837, 0.1717, 0.1599, 0.2414, 0.2434]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "y_hat = F.softmax(z, dim=1) # 열방향에 대해서\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 4])\n",
      "tensor([[3],\n",
      "        [1],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y) # (3,)\n",
    "print(y.unsqueeze(1)) # (3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(y_hat)\n",
    "# .scatter_(dim, tensor, 채울 값)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1),1)\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5147, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss = (y_one_hot * -torch.log(y_hat)).sum(dim=1).mean()\n",
    "nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀의 loss function 구현하기 (high level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. F.softmax() + torch.log() = F.log_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0866, 0.3617, 0.6359, 0.8554, 0.5943],\n",
      "        [0.2856, 0.8149, 0.2532, 0.4322, 0.0531],\n",
      "        [0.9548, 0.8600, 0.5900, 0.0558, 0.1975]], requires_grad=True) \n",
      "\n",
      "tensor([[-2.0627, -1.7876, -1.5134, -1.2940, -1.5550],\n",
      "        [-1.7257, -1.1964, -1.7581, -1.5791, -1.9582],\n",
      "        [-1.2470, -1.3418, -1.6118, -2.1459, -2.0042]], grad_fn=<LogBackward>)\n",
      "tensor([[-2.0627, -1.7876, -1.5134, -1.2940, -1.5550],\n",
      "        [-1.7257, -1.1964, -1.7581, -1.5791, -1.9582],\n",
      "        [-1.2470, -1.3418, -1.6118, -2.1459, -2.0042]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True) # softmax들어가기전\n",
    "print(z,'\\n') \n",
    "print(torch.log(F.softmax(z, dim=1)))\n",
    "# 둘이 같음\n",
    "print(F.log_softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. F.log_softmax() + F.nll_loss() = F.cross_entropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6181, grad_fn=<MeanBackward0>)\n",
      "tensor(1.6181, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6181, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print((y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean())\n",
    "print(F.nll_loss(F.log_softmax(z, dim=1), y)) # negative log likelihood\n",
    "print(F.cross_entropy(z, y))\n",
    "# F.cross_entropy는 비용 함수에 소프트맥스 함수까지 포함하고 있음!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 구현하기 (low level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]] \n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0] # label=3가지\n",
    "x_train = torch.FloatTensor(x_train) # (8,4)\n",
    "y_train = torch.LongTensor(y_train) # (,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.unsqueeze(1).shape)\n",
    "y_train.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 1.0986\n",
      "EPOCHS:  200 | loss: 0.6900\n",
      "EPOCHS:  400 | loss: 0.6041\n",
      "EPOCHS:  600 | loss: 0.5339\n",
      "EPOCHS:  800 | loss: 0.4669\n",
      "EPOCHS: 1000 | loss: 0.4000\n"
     ]
    }
   ],
   "source": [
    "w = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = optim.SGD([w,b], lr=0.1)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    y_hat = F.softmax(x_train.matmul(w) + b, dim=1)\n",
    "    nll_loss = (y_one_hot * -torch.log(y_hat)).sum(dim=1).mean()\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    nll_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, nll_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 구현하기(high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 1.0986\n",
      "EPOCHS:  200 | loss: 0.6900\n",
      "EPOCHS:  400 | loss: 0.6041\n",
      "EPOCHS:  600 | loss: 0.5339\n",
      "EPOCHS:  800 | loss: 0.4669\n",
      "EPOCHS: 1000 | loss: 0.4000\n"
     ]
    }
   ],
   "source": [
    "w = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = optim.SGD([w,b], lr=0.1)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    y_hat = x_train.matmul(w) + b\n",
    "    # cross_entropy == logsoftmax + nllloss\n",
    "    cross_entropy = F.cross_entropy(y_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cross_entropy.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, cross_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 nn.Module로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 1.5557\n",
      "EPOCHS:  200 | loss: 2.0962\n",
      "EPOCHS:  400 | loss: 1.9396\n",
      "EPOCHS:  600 | loss: 5.5022\n",
      "EPOCHS:  800 | loss: 0.0064\n",
      "EPOCHS: 1000 | loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(4,3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    y_hat = model(x_train)\n",
    "    loss = F.cross_entropy(y_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 클래스로 구현하기 (1)\n",
    "- linear + logsoftmax + nllLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_softmax_regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        self.logSoftmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.logSoftmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 1.3507\n",
      "EPOCHS:  200 | loss: 0.1728\n",
      "EPOCHS:  400 | loss: 0.0817\n",
      "EPOCHS:  600 | loss: 0.0477\n",
      "EPOCHS:  800 | loss: 0.0313\n",
      "EPOCHS: 1000 | loss: 0.0221\n"
     ]
    }
   ],
   "source": [
    "model = Log_softmax_regression()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    y_hat = model(x_train)\n",
    "    loss = F.nll_loss(y_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 클래스로 구현하기 (2)\n",
    "- linear + cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 1.6434\n",
      "EPOCHS:  200 | loss: 0.1784\n",
      "EPOCHS:  400 | loss: 0.0858\n",
      "EPOCHS:  600 | loss: 0.0507\n",
      "EPOCHS:  800 | loss: 0.0334\n",
      "EPOCHS: 1000 | loss: 0.0237\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    y_hat = model(x_train)\n",
    "    loss = F.cross_entropy(y_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀 클래스로 구현하기 (2) with mini_batch\n",
    "- linear + cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:    0 | loss: 3.5215\n",
      "EPOCHS:  200 | loss: 0.8962\n",
      "EPOCHS:  400 | loss: 0.8009\n",
      "EPOCHS:  600 | loss: 0.6501\n",
      "EPOCHS:  800 | loss: 0.5489\n",
      "EPOCHS: 1000 | loss: 0.3493\n"
     ]
    }
   ],
   "source": [
    "model = Linear_model()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "EPOCHS = 1000\n",
    "for i in range(EPOCHS + 1):\n",
    "    for mini_batch_idx, mini_batch in enumerate(dataloader):\n",
    "        x, y = mini_batch\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 200 == 0:\n",
    "        print(\"EPOCHS: {:4d} | loss: {:.4f}\".format(i, loss))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀로 MNIST 데이터 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "data_loader = DataLoader(dataset = mnist_train,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True,\n",
    "                         drop_last = True) # 마지막 배치 버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: MNIST_data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor() \n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: MNIST_data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.dataset, '\\n')\n",
    "print(mnist_train)\n",
    "# data_loader.dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([128, 1, 28, 28])\n",
      "y.shape: torch.Size([128])\n",
      "reshaped_x: torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for x, y in data_loader:\n",
    "    if cnt == 0:\n",
    "        print(\"x.shape:\", x.shape)\n",
    "        print(\"y.shape:\", y.shape)\n",
    "        \n",
    "        x = x.view(-1, 28*28)\n",
    "        print(\"reshaped_x:\", x.shape)\n",
    "        \n",
    "    cnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model + train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(784, 10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS:  0 | loss: 0.36160\n",
      "EPOCHS:  1 | loss: 0.29267\n",
      "EPOCHS:  2 | loss: 0.28524\n",
      "EPOCHS:  3 | loss: 0.27871\n",
      "EPOCHS:  4 | loss: 0.27649\n",
      "EPOCHS:  5 | loss: 0.27381\n",
      "EPOCHS:  6 | loss: 0.26848\n",
      "EPOCHS:  7 | loss: 0.26838\n",
      "EPOCHS:  8 | loss: 0.26868\n",
      "EPOCHS:  9 | loss: 0.26540\n",
      "EPOCHS: 10 | loss: 0.26407\n",
      "EPOCHS: 11 | loss: 0.26311\n",
      "EPOCHS: 12 | loss: 0.26293\n",
      "EPOCHS: 13 | loss: 0.26221\n",
      "EPOCHS: 14 | loss: 0.26005\n"
     ]
    }
   ],
   "source": [
    "model = my_model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "crit = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    num_of_batch = len(data_loader)\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        x = x.view(-1, 784).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = crit(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / num_of_batch\n",
    "        \n",
    "    print(\"EPOCHS: {:2d} | loss: {:.5f}\".format(i, avg_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|pred|: torch.Size([10000, 10])\n",
      "tensor([7, 2, 1,  ..., 4, 5, 6], device='cuda:0')\n",
      "Accuracy:  0.8833999633789062\n",
      "----------------------------------\n",
      "\n",
      "Label: 1\n",
      "pred: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "    x_test = mnist_test.data.view(-1, 28*28).float().to(device)\n",
    "    y_test = mnist_test.targets.to(device)\n",
    "    pred = model(x_test)\n",
    "    correct_pred = torch.argmax(pred, 1) == y_test\n",
    "    accuracy = correct_pred.float().mean()\n",
    "    print(\"|pred|:\", pred.shape)\n",
    "    print(torch.argmax(pred, 1)) # 열을따라서, max인 idx저장\n",
    "    print('Accuracy: ', accuracy.item())\n",
    "    print('----------------------------------\\n')\n",
    "    \n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    x_test_one = mnist_test.data[r].view(-1, 28*28).float().to(device)\n",
    "    y_test_one = mnist_test.targets[r].to(device)\n",
    "    pred_one = model(x_test_one)\n",
    "    print('Label:', y_test_one.item())\n",
    "    print('pred:', torch.argmax(pred_one, 1).item())\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
