{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환 신경망(Recurrent Neural Network, RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "# 10개의 단어로 이루어진 4개의 문장\n",
    "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[0.99990111 0.99999922 0.99720703 0.99971953 0.9997697  0.99996462\n",
      "  0.99680481 0.99991553]\n",
      " [0.99929264 0.99999509 0.99787489 0.99923395 0.9994951  0.99992921\n",
      "  0.99640098 0.99980596]\n",
      " [0.99985981 0.99999913 0.99854879 0.99973505 0.99992402 0.99997632\n",
      "  0.99643223 0.99991811]\n",
      " [0.99988246 0.99999924 0.99834738 0.99977907 0.99990402 0.99998148\n",
      "  0.99655138 0.99993118]\n",
      " [0.99946168 0.99999708 0.99893958 0.99950283 0.99988163 0.99996336\n",
      "  0.99645762 0.99986712]\n",
      " [0.99922305 0.99999355 0.99629732 0.99898713 0.99950165 0.99993903\n",
      "  0.99251906 0.99967277]\n",
      " [0.99967645 0.99999756 0.99733963 0.99957721 0.99950972 0.99996588\n",
      "  0.99642565 0.99988265]\n",
      " [0.99940867 0.99999522 0.99659329 0.99919708 0.99960287 0.99995178\n",
      "  0.99302738 0.99973855]\n",
      " [0.99972884 0.99999823 0.99838149 0.99949271 0.99984296 0.99993486\n",
      "  0.99682101 0.99986194]\n",
      " [0.9996353  0.99999759 0.99833226 0.99933872 0.99976982 0.99989911\n",
      "  0.99712118 0.99983353]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "for input_t in inputs: # 한 행 = input_t = 단어하나의 벡터\n",
    "    hidden_state_t_after = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t))\n",
    "    total_hidden_states.append(list(hidden_state_t_after))\n",
    "    hidden_state_t = hidden_state_t_after\n",
    "    # output = f(hidden_state_t*weight + b)\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
    "print(len(total_hidden_states))\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치의 nn.RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape) # 전체 time step의 결과\n",
    "print(_status.shape) # 마지막 time step의 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape) # 전체 time step의 결과\n",
    "# (bs, time_step_size(=word_length), hidden_size\n",
    "\n",
    "print(_status.shape) # 마지막 time step의 결과\n",
    "# (num_layers, bs, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
    "- 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape) # 전체 time step의 결과\n",
    "# (bs, time_step_size(=word_length), hidden_size*2\n",
    "\n",
    "print(_status.shape) # 마지막 time step의 결과\n",
    "# (num_layers*2, bs, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바닐라 RNN의 한계\n",
    "- 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점\n",
    "- (time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "cell = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape) # 전체 time step의 결과\n",
    "# (bs, time_step_size(=word_length), hidden_size*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
