{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 RNN(Char RNN)\n",
    "- RNN의 입출력의 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['!', 'a', 'e', 'l', 'p']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!ep'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 8\n",
    "output_size = 7\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c,i) for i, c in enumerate(char_vocab))\n",
    "print(char_to_index)\n",
    "\n",
    "index_to_char = {} # 마지막에 다시 변환해주기 위함\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 4, 4, 3, 2], [4, 4, 3, 2, 0, 2, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = [char_to_index[i] for i in input_str]\n",
    "y_data = [char_to_index[i] for i in label_str]\n",
    "x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 4, 4, 3, 2]], [[4, 4, 3, 2, 0, 2, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "\n",
    "x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.eye -> identity행렬 만듦\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data] \n",
    "x_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. model 만들고 train하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(x.shape) # (bs, in_size, word_vec_size)\n",
    "        x, _status = self.rnn(x)\n",
    "#         print(x.shape) # (bs, in_size, hidden_size)\n",
    "        x = self.fc(x)\n",
    "#         print(x.shape) # (bs, in_size, out_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "y_hat = net(X)\n",
    "y_hat.shape # (bs, timesteps=input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n"
     ]
    }
   ],
   "source": [
    "print(y_hat.view(-1, input_size).shape) \n",
    "# (output_size, bs+timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.5952900648117065 prediction str:  pll!pep \n",
      " prediction:  [[4 3 3 0 4 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "1 loss:  1.3477944135665894 prediction str:  p!l!pep \n",
      " prediction:  [[4 0 3 0 4 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "2 loss:  1.000227689743042 prediction str:  pplepep \n",
      " prediction:  [[4 4 3 2 4 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "3 loss:  0.650600254535675 prediction str:  pplepep \n",
      " prediction:  [[4 4 3 2 4 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "4 loss:  0.4056902527809143 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "5 loss:  0.25015988945961 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "6 loss:  0.14836657047271729 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "7 loss:  0.08627326041460037 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "8 loss:  0.05114513263106346 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n",
      "9 loss:  0.03092355653643608 prediction str:  pple!ep \n",
      " prediction:  [[4 4 3 2 0 2 4]] true Y:  [[4, 4, 3, 2, 0, 2, 4]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    \n",
    "#     print(outputs.shape) # (bs, time_step_size(=input_size), output_size)\n",
    "#     print(Y.shape) # (bs, output_size)\n",
    "#     print(outputs.view(-1, input_size).shape)\n",
    "#     print(Y.view(-1).shape)\n",
    "    \n",
    "    loss = criterion(outputs, Y)\n",
    "    # view -> 배치차원 제거\n",
    "#     loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#     print(outputs.data.shape) # (bs, time_step_size(=input_size), output_size)\n",
    "    result = outputs.data.numpy().argmax(axis=1)\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction str: \", result_str, '\\n', \"prediction: \", result, \"true Y: \", y_data, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 RNN(Char RNN) - 더 많은 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 0, 'g': 1, 'f': 2, 'i': 3, 'e': 4, 'p': 5, 'o': 6, 'y': 7, 'd': 8, 'm': 9, \"'\": 10, 'b': 11, 'l': 12, ',': 13, 'w': 14, 'a': 15, 'k': 16, 'r': 17, ' ': 18, 'u': 19, 'c': 20, 't': 21, '.': 22, 'h': 23, 's': 24}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = dic_size\n",
    "sequence_length = 10  # 임의 숫자 지정\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence) , sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i : i+sequence_length]\n",
    "    y_str = sentence[i+1 : i+sequence_length+1]\n",
    "    if i == 0:\n",
    "        print(i, x_str, '->', y_str)\n",
    "    \n",
    "    x_data.append([char_dic[c] for c in x_str]) # x str to idx\n",
    "    y_data.append([char_dic[c] for c in y_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 170 ,  10 )\n",
      "( 170 ,  10 )\n",
      "[3, 2, 18, 7, 6, 19, 18, 14, 15, 0]\n",
      "[2, 18, 7, 6, 19, 18, 14, 15, 0, 21]\n"
     ]
    }
   ],
   "source": [
    "print(\"(\", len(x_data),\", \", len(x_data[0]), \")\")\n",
    "print(\"(\", len(y_data),\", \", len(y_data[0]), \")\")\n",
    "print(x_data[0]) # if you wan에 해당됨.\n",
    "print(y_data[0]) # f you want에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot) # one hot encoded\n",
    "Y = torch.LongTensor(y_data) # one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x, _status = self.rnn(x)\n",
    "#         print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ygllllllllllllllllllylllllllyllllllllyllllllllllllllllllllllllllllllyllllllllllllllllllylllllllllllllyyllllllllyllylllllllllllllyllllllllllllllllllllllllllllylllllllllllllllllllll \n",
      "\n",
      "       w                                                  w                                                                w                                                        \n",
      "\n",
      "                                                                                                                                                                                    \n",
      "\n",
      "ew.hewooe..ssssss.essssseg.sas.es.msmssssessssssssssssssssssswssmssssssessssessssess.ssesseees.ssm.sss.sees.esss.sssssesesessesmsmsesssegmes.sssssssesssesees.sssssessssmssssemsses \n",
      "\n",
      "oooooomooomoooooomoooooooooooomoomoooooooooooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooaoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo \n",
      "\n",
      "tott oot oooooooooooooooooloooooooooooooooooooooooooooooolooooloooooooooooooooooooooooooooooooooooooooooooooooooooooooooooloooooooooooooooooooooooooooooooooooooooooooooloooooooooo \n",
      "\n",
      "t   t  osm m totm llt tol tl llot l ut   mlm lllttlm tm  tt e l  lut tt   loe t eouoeotm  lt lt toemt touotoeototo ot u ettt t l tolt lo lm eluo lt eotoe to tmtll  le ll  toet ll  \n",
      "\n",
      "t  tttt  elt tett t t towetotetetttowe t t t  twee eetotewe tttewe tutetettee totewotowott e tett t totedstotewototttoboue t t t totewtou toeetotet tetetowetotewttdtt eetttttet te \n",
      "\n",
      "e    t   tot t  t t t to  tototow towt t   nt   ttot te t t t   et t to n tt  t te oto t n t tt t ttt wt  to o otowot t tt n t t t wt t    t twne t  otede e  t tnt t   t  to t  tn \n",
      "\n",
      "e    tw ttot  t t t t e e o w t w    n t       w  o    t  to   ee w  w e  t t t e w t e w  to toto    w e e e w t t eo    ototw ew e ea e     t twt e  nd w w     t  ww  o t e w    \n",
      "\n",
      "eaa  ta et toe  t t t e e o w e t e etotoea a  e to    a etor  ee    w erot t t e e e e e  no toto t  w d e e e w e t toerotod  et e ea e  rd w e t e a e e e t e e  e  toet e e    \n",
      "\n",
      " a    aaedodoe    t t eoe d dot toe et tota   or toe aoeheto   re eo t e  e t tot eot ehea to  odoeoeot e eoe dod e t toeo toe e toe ea e eod eod toe eod t totot to e  io aoe eot  \n",
      "\n",
      " a o lto totoemotleototoe n d d toeoetoto a e'oroto  omonoto d ed io toedoto otonr oe oie  ahenoto  o t t toe t wot totoe' to g et e'otoedord doroto'g  n e  at  o o to ae to  tie  \n",
      "\n",
      " si r to t to to lt toto  t d d s e  totp t  m n to gsoem to dorg is do ' t s ton'tot eier toe' to  s t d toe't w t d to n to g  t em to mo d tom toe's d t t t d t  to tl to mtot  \n",
      "\n",
      " sm r t  t tontst t t tot t d t t t stot s g m nlt e d emst  ms g rs doe gt t t d t t s   nt e gt sis d d t e t d t d t   cto ms d e wsosms d t d toe d d t d t t t  tm,og to  d t  \n",
      "\n",
      " sm r t  m to m i t t sme s d emt e  mstp ig d r t  mt er t  ms g  s toem t  mt emt tmsin  toem tosms t s to mt d t d soe  t ems t e  to ms d t n t  mtod t s t   tm t  i  t emt dk \n",
      "\n",
      " si r t  t to m t t tosmers w ems tm t tp rg r r t  ms er to mo  e s w  m t   toemt tm i e t em to ms t d to mt d t tosoer t emo t em to m r em t t emtod emsot      eo i  toemt dk \n",
      "\n",
      " ttoe to t toemur t tot ert w emt to b tp ng d   t  mt er to mo ge s woem t s toemt tc ig  toer tosms t d to  t d t t s er toemo d em to mo g tot toemtod eksot      toon  toemt wk \n",
      "\n",
      " ntoe to t toebur t tot erd wodlt tp b tp ng r  ct dkt er toeco g ri woeb t d woekt tn  i  toer tosio t d doekt but tod ec toeco d ec ao bo g tot toertod e sot    e toong toept t  \n",
      "\n",
      "lntoe to t toeboa r tot er, boelt an b tp al r  ltonlt er toelo l rtowo b tnd wo lt er iin toer aosio t d wo k, but tod er toelt toe  aodco g tor toertod drioa    n thhag toept an \n",
      "\n",
      "lntoe do d to bup d tothep, doe'tha  b tp ag r   to 'thek to do l ri to l and to 'thtk ie  toek theio tnd to ', but tothep toelo dhek to bo l tor toept d eks t    g ti a  theit dk \n",
      "\n",
      "dnthek,o d to dup d toth,r, doe'tht  p tp ngor   to 'thek toedo g rt wo d and to 't t  ee  thek tosio tnd to ', but dather to so thep to do g tor toe t d eriet   n  ta tg theit tk \n",
      "\n",
      "dnthnk,ond to dup d tnth,r, don't ep p np ngor   ton'thep to to gert wo d tnd to 't er ee  ther tosi, tnd do ',nbut dather tonmo ther tonco g dor toertnd eriet   n  ta ag thess tn \n",
      "\n",
      "d thr to d to cui d tnth,r, don't ep m np neor   thnethep thnco gers do c and wo 't emsigr them thsi, tnd do k,ndut dather thnmo them th cong dor toemsnd ersetn  n stysag thems tc \n",
      "\n",
      "lsshr tond to cui t tnther, dog't am m up neor e together thnco lems wooc and won't amsigr them tosi, tnd wo k, but dather thnco them toscong for toemsnd ersetm en ssy ag themsitn \n",
      "\n",
      "lsthr tond to bum d anther, don't amum up nior e togethem thnco lemt tooc and wo 't amsigr them tosks tnd do k, but dather thnco them toncong tor themsnd emsitn en sty ag themsitn \n",
      "\n",
      "l the tont to but d anther, don't am m up neor e together tonco eent toop and won't amsigr them tosks and ao k, but rather tonco them to cong tor themsnd ersetn en it  ap themsetn \n",
      "\n",
      "l the tont to bui d anther, don't dmum up peorle together tonbo lent toop and wo 't amsig  them aoske and wo k, but rather tonch ther to bong tor themsnd ensitn en ity op thersean \n",
      "\n",
      "l toa tont to bui d ansher, bon't drum up peorle together tonco lent wo p and don't dmsig  them toske and do k, but rather tonch them to bong  or themsnd ens tn en ity op thers tn \n",
      "\n",
      "l tha wont to bui d ansher, don't drut up peorle togethem tonco lent wool and won't disign them toske and wo k, but rather toach ther to bong  or themsnd ens in en ity op thens tn \n",
      "\n",
      "g tha wont to bui d ansher, don't drut up peorle tonether tonco lent wool and won't aisign them tosks tnd wo k, but rather thach ther to long for themsnd ens in en ity op thersitn \n",
      "\n",
      "g t e wont to bui d ansher, don't drui up peorle together tenco gent wood and don't disign them tasks and do k, but rather teach them ta long for themsnd ens in en ity op thems tn \n",
      "\n",
      "g t e wont to bui d ansher, don't drui up peorle together tenco lent wood and don't aisign them tasks and wo k, but rather teach them ta long for themsnd ens in en ity of theme tn \n",
      "\n",
      "g t r tont to build anshir, don't arum up peorle together tenco lent wood and don't amsign them tosks and wo k, but rather teach them to long for themsnd e s an en ity of themsetn \n",
      "\n",
      "g thr wont to build anshir, don't arum up peorle together tonco gent wood and don't amsign them tosks and wo k, but rather toach them to long for themsndle s im en ity of themsian \n",
      "\n",
      "g thr want to build anshir, don't drum up people together toncolgect wood and don't dmsign them tosks and work, but rather toach them ta long for themsndle s im ensity of themsian \n",
      "\n",
      "g thr want to build anshir, don't drum up people together toncollect wood and don't dssign them tosks and work, but rather toach them ta long for themsndle s im ensity of the sean \n",
      "\n",
      "g yhr want to build anshir, don't drum up people together to co eect wood and don't assign them tosks and work, but rather toach them ta long for themsnd e s im ensity of the sean \n",
      "\n",
      "g yhr want to build anship, don't drum up people together to colgect wood and don't assign them tosks and work, but rather toach them to long for themendle s ir ensity of the eean \n",
      "\n",
      "g yor wont to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for themendle s ir ensity of the eian \n",
      "\n",
      "g yor wont to build anship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endle s ir ensity of the eean \n",
      "\n",
      "g you want to build anship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the end e s im ensity of the eean \n",
      "\n",
      "g y u want to build anship, don't drum up people to ether te collect wood and don't assign the  tosks and work, but rather teach them ta long for the end ems ir ensity of the e an \n",
      "\n",
      "g you want to build anship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to bong for themend e s i mensity of the e an \n",
      "\n",
      "g you want to build anship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eian \n",
      "\n",
      "g you want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eian \n",
      "\n",
      "g you want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the s an \n",
      "\n",
      "g you want to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them ta bong for the endless immensity of the sian \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the eean \n",
      "\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the eian \n",
      "\n",
      "g y u want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the eean \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sean \n",
      "\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sian \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the sean \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sean \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sean \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sean \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sean \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together toncollect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but aather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "deeper_net = Net(dic_size, hidden_size, 2)\n",
    "optimizer = optim.Adam(deeper_net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "#     print(X.shape)\n",
    "    outputs = deeper_net(X) \n",
    "#     print(outputs.shape)\n",
    "#     print(Y.shape)\n",
    "#     print(outputs.view(-1, dic_size).shape)\n",
    "#     print(Y.view(-1).shape)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    results = outputs.argmax(dim=2)\n",
    "#     print(results)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "    \n",
    "    print(predict_str, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 단위 RNN - 임베딩 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'best', 'medicine', 'Repeat', 'memory', 'for', 'is']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentence = \"Repeat is the best medicine for memory\".split()\n",
    "\n",
    "vocab = list(set(sentence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'best': 2, 'medicine': 3, 'Repeat': 4, 'memory': 5, 'for': 6, 'is': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
    "word2index['<unk>']=0\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'best', 3: 'medicine', 4: 'Repeat', 5: 'memory', 6: 'for', 7: 'is', 0: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
    "index2word = {v: k for k, v in word2index.items()}\n",
    "print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [4, 7, 1, 2, 3, 6, 5]\n",
      "input: tensor([[4, 7, 1, 2, 3, 6]])\n",
      "output: tensor([[7, 1, 2, 3, 6, 5]])\n"
     ]
    }
   ],
   "source": [
    "def build_data(sentence, word2index):\n",
    "    encoded = [word2index[token] for token in sentence]\n",
    "    print(\"encoded:\", encoded)\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:]\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0)\n",
    "    return input_seq, label_seq\n",
    "\n",
    "X, Y = build_data(sentence, word2index)\n",
    "print(\"input:\", X)\n",
    "print(\"output:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
    "                                            \n",
    "                                            embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
    "                                batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        \n",
    "        output = self.embedding_layer(x)\n",
    "#         print(output.shape)\n",
    "        \n",
    "        output, hidden = self.rnn_layer(output)\n",
    "#         print(output.shape)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "#         print( output.view(-1, output.size(2)).shape)\n",
    "        return output.view(-1, output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/201] 2.1626 \n",
      "Repeat <unk> Repeat for Repeat <unk> <unk>\n",
      "\n",
      "[41/201] 1.5776 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[81/201] 0.9204 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[121/201] 0.4742 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[161/201] 0.2519 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[201/201] 0.1478 \n",
      "Repeat is the best medicine for memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
    "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
    "hidden_size = 20  # RNN의 은닉층 크기\n",
    "\n",
    "# 모델 생성\n",
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "# 손실함수 정의\n",
    "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
    "# 옵티마이저 정의\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "decode = lambda y: [index2word.get(x) for x in y]\n",
    "\n",
    "for step in range(201):\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 순방향 전파\n",
    "    output = model(X)\n",
    "    # 손실값 계산\n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    # 역방향 전파\n",
    "    loss.backward()\n",
    "    # 매개변수 업데이트\n",
    "    optimizer.step()\n",
    "    # 기록\n",
    "    if step % 40 == 0:\n",
    "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
